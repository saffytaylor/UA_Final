{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551ff7fe-0528-44a3-9cf9-e35a6a896872",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "---\n",
    "\n",
    "Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdefca9f-02bf-4a7b-9a0d-176ecc1dba2a",
   "metadata": {},
   "source": [
    "## Cleaning for Spatial microsimulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1855fd5-f0e4-4ad0-9b11-98e5042bafe6",
   "metadata": {},
   "source": [
    "### Step 1: Importing in the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c85ee-f38e-4f5f-a6fb-889272242255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf54b7d-e3f1-4967-9b0c-64128b25d5d8",
   "metadata": {},
   "source": [
    "### Step 2: Reading in the CSV we made in RStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543a557-fd00-4a35-b623-231bd8d9e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/BSA2020/bsa2020.csv\"\n",
    "BSA_2020 = pd.read_csv(csv_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11192e1-f8d8-47ce-879d-dd3466db0578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BSA_2020.head() # checking our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a0224-2ffc-4a64-acb9-6e2273f6087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020.shape #Â checking the shape of the dataframe - it's all looking good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d475f-beb0-4de7-a6da-10bfda3af87a",
   "metadata": {},
   "source": [
    "### Step 3: Subsetting\n",
    "To be able to create a synthetic population of York, we only want data entries from Yorkshire and Humberside, and we do not need all of these columns, so we will subset for just what we are after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f7120-16bd-4f9a-b02e-ac1e637ee23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting for just the Yorkshire and Humberside = indicated with by the value of GOR being 3\n",
    "yorkshire_bsa = BSA_2020.loc[(BSA_2020.GOR == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc235f-7c0f-40ea-b474-e4db3c0e1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "yorkshire_bsa.shape # that's cut our sample down nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f6042-b119-4f8b-8d63-2c0286eb8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting to only include a fraction of the columns available\n",
    "keep_columns = ['RespAgeE', # age a last birthday, capped at 80+\n",
    "                'RespSx2cat', # respondants sex\n",
    "                'REconSum20', # economic activity\n",
    "                'SupParty', # do they support a party\n",
    "                'PARTYFW', # which party do they follow\n",
    "                'Politics', # do they have an interest in politics\n",
    "                'welfgrp', # opinions on the welfare system\n",
    "                'Redistrb', # pro wealth redistribution?\n",
    "                'leftrigh', # left or right leaning (scaled)\n",
    "                'leftrig2', # left or right leaning (grouped)\n",
    "                'libauth', # liberal vs authoritarian (scaled)\n",
    "                'libauth2', # liberal vs authoritarian (grouped)\n",
    "                'ReligSum20', # do they follow a religion\n",
    "                'BestNatU2', # what nationality best describes them\n",
    "                'RaceOri4', # what race best describes them\n",
    "                'DisActDV', # do they have a long-term condition or disability\n",
    "                'Voted', # did they vote in the last general election\n",
    "                'Vote', # who did they vote for in the last general election\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208e700-588a-4ce8-9156-ebe24f1d007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to subset columns\n",
    "def subset_dataframe(dataframe, keep_columns):\n",
    "    subset_bsa = pd.DataFrame() # initialises a new results dataframe\n",
    "    for x in keep_columns: # for loop add columns to new dataframe\n",
    "        subset_bsa[x] = dataframe[x]\n",
    "        \n",
    "    return subset_bsa # returns the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f561e-ab84-4218-9fe7-de09c342c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bsa = subset_dataframe(yorkshire_bsa, keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774813a-e823-499d-928a-7907bb24468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bsa.shape # we've subset down to a total of 18 columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba43d2f-fb94-4375-8016-2442749a7595",
   "metadata": {},
   "source": [
    "Now we have a nice little subset, we'll export that as a CSV so we don't have to run any of this code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e8be1-5793-4a10-8037-ab7b7caf3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bsa.to_csv(\"data/BSA2020/subset4_regional_bsa_data.csv\", index=False)\n",
    "#hashtagged out as we have now run this and have no need to run it again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8e3a6-9bb8-4362-a2a9-744f536cb82f",
   "metadata": {},
   "source": [
    "### Step 4: Cleaning the new CSV\n",
    "we don't want to have to have any NaN values so we will clean this dataset to ensure that we remove rows with missing entries so we have a complete dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d00386-b8a6-44a1-b498-1480024dab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading back in the CSV so we don't have to rerun all the steps above\n",
    "csv_path_2 = \"data/BSA2020/subset4_regional_bsa_data.csv\"\n",
    "subset_bsa_cleaning = pd.read_csv(csv_path_2, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2381df2-723e-4c99-acc2-c06079cd164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bsa_cleaning.head() # from just checking the head we can already see columns with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723c1fe-8b52-434a-9d79-da97aa366499",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_cleaned = subset_bsa_cleaning.dropna() # this gives us a cleaned dataset with no NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21da607-59b3-4a27-857a-6bab1e83d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index of this data\n",
    "bsa_cleaned.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7e8e6-09a2-4d43-ae47-89b824a132e1",
   "metadata": {},
   "source": [
    "Next we'll rename some of the columns to more descriptive names just to help for analysis later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a235c24-ef82-4ad4-9c61-679861fce198",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c33b42-98e9-48ae-85e9-78a641aa5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    'RespAgeE': 'age', # age of respondant at last birthday (capped at 80)\n",
    "    'RespSx2cat': 'sex', # sex of respondant\n",
    "    'REconSum20': 'economic', # economic activity of respondent\n",
    "    'SupParty': 'partySup', # does respondent support a particular party\n",
    "    'PARTYFW': 'partySupWho', # which party do they support (if they had to choose on day of survey)\n",
    "    'Politics': 'polInterest', # do they have an interest in politics\n",
    "    'welfgrp': 'welfare', # opinions on the welfare system\n",
    "    'Redistrb': 'redistrb', # pro wealth redistribution?\n",
    "    'leftrigh': 'leftright', # left or right leaning (scaled)\n",
    "    'leftrig2': 'leftright2', # left or right leaning (grouped)\n",
    "    'libauth': 'libauth', # liberal vs authoritarian (scaled)\n",
    "    'libauth2': 'libauth2', # liberal vs authoritarian (grouped)\n",
    "    'ReligSum20': 'religion', # what religion do they follow\n",
    "    'BestNatU2': 'nationality', # what nationality best describes them\n",
    "    'RaceOri4': 'raceOrigin', # which racial group do they best identify with\n",
    "    'DisActDV': 'disability', # do they have a long-term condition or disability\n",
    "    'Voted': 'voteAct', # did they vote in the last general election\n",
    "    'Vote': 'voteParty', # who did they vote for in the last general election\n",
    "}\n",
    "\n",
    "bsa_cleaned = bsa_cleaned.rename(columns=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3550a-83b7-4e2b-944a-e6ed83e2e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_cleaned.head() # looking beautiful and easy to interpret!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1eacc7-56ad-4b73-af28-3c3d3b020210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an id column quickly!\n",
    "bsa_cleaned.insert(0, 'id', range(1000, 1000 + len(bsa_cleaned)))\n",
    "bsa_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf33d9-2671-4d8c-b3db-7bcf8a6d9b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping individuals who refused to answer what age they are\n",
    "bsa_cleaned['age'].unique()\n",
    "bsa_cleaned = bsa_cleaned.drop(bsa_cleaned[bsa_cleaned['age'] == 999].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a3cc5-fd63-464c-8d60-1ffbc20c30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_cleaned['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4c264-b102-4f23-9bed-7983110ede06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_cleaned.to_csv(\"data/BSA2020/final_bsa_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabc642-069d-43ea-92ac-446793fc22c5",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410248c-2023-4891-9384-b3d6c7a3db55",
   "metadata": {},
   "source": [
    "## Cleaning for Geodemographics\n",
    "\n",
    "To be able to carry out a geodemographic clustering, our data needs to be in the form of aggregate counts per zone. At the moment each row represents an individual in York. We need each row to represent a zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4ce69-4c0e-4eb7-9e72-cde06a4f6b55",
   "metadata": {},
   "source": [
    "### Step 1: Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea561bc3-8f27-48c5-b1f3-fdf45003d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6c6cf-006b-4df9-9a86-6ec2b0c8f724",
   "metadata": {},
   "source": [
    "### Step 2: Reading in the individual level data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf6a43-ea64-4ae0-b499-96c04fc9598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "york_BSA = pd.read_csv('data/BSA_synthetics_geo.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d420fe8-b032-4027-afe7-2781290bcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "york_BSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965b725-abcd-473f-95e6-ddadb529d3ae",
   "metadata": {},
   "source": [
    "### Step 3: Creating a new database of aggregate counts\n",
    "After exploring a few options on how best to transform the dataset, I decided that the best method was to flatten the dataset according to the columns I wanted, to then be able to sum the rows and save them into a new aggregate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4053353-feae-4acb-b65e-cd19f0c63de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this created a new dataframe where the first row is each of our electoral zone codes\n",
    "BSA_zones = pd.DataFrame({'geo_code': york_BSA['geo_code'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74061cdf-6b4c-4f8c-b0cb-3834f58be46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the flattened dataframes to sum our aggregates from\n",
    "age_flat = pd.pivot_table(york_BSA,columns=['age'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "sex_flat = pd.pivot_table(york_BSA,columns=['sex'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "partySup_flat = pd.pivot_table(york_BSA,columns=['partySup'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "partySupWho_flat = pd.pivot_table(york_BSA,columns=['partySupWho'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "polInterest_flat = pd.pivot_table(york_BSA,columns=['polInterest'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "welfare_flat = pd.pivot_table(york_BSA,columns=['welfare'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "redistrb_flat = pd.pivot_table(york_BSA,columns=['redistrb'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "leftright2_flat = pd.pivot_table(york_BSA,columns=['leftright2'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "libauth2_flat = pd.pivot_table(york_BSA,columns=['libauth2'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "welfare_flat = pd.pivot_table(york_BSA,columns=['welfare'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "redistrb_flat = pd.pivot_table(york_BSA,columns=['redistrb'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "voteParty_flat = pd.pivot_table(york_BSA,columns=['voteParty'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "religion_flat = pd.pivot_table(york_BSA,columns=['religion'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "nationality_flat = pd.pivot_table(york_BSA,columns=['nationality'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "raceOrigin_flat = pd.pivot_table(york_BSA,columns=['raceOrigin'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)\n",
    "disability_flat = pd.pivot_table(york_BSA,columns=['disability'],values='id', index='geo_code', aggfunc=len, fill_value=0, observed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaca571-c51a-465e-a804-640a1464f7f4",
   "metadata": {},
   "source": [
    "Now we've got flatten datasets pivoted so that our index rows are geo_codes, we can use this to calculate our aggregate data into new columns and put them in our new dataset!\n",
    "\n",
    "The follow cell could have likely been made to be more efficient using functions, however as the groupings changed depending on the category, doing it individually felt like the least time-consuming option so that we could move onto geodemographics. There is definitely room for improvement in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2baae-ac83-4d20-badc-6caea1d90e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general population aggregates\n",
    "BSA_zones['total_pop'] = age_flat.sum(axis=1).tolist()\n",
    "BSA_zones['age1864'] = age_flat.loc[:,'20':'64'].sum(axis=1).tolist()\n",
    "BSA_zones['age65'] = age_flat.loc[:,'65':].sum(axis=1).tolist()\n",
    "BSA_zones['female'] = sex_flat[1].tolist()\n",
    "BSA_zones['male'] = sex_flat[2].tolist()\n",
    "\n",
    "# do they support a particular party\n",
    "BSA_zones['support_yes'] = partySup_flat[1].tolist()\n",
    "BSA_zones['support_no'] = partySup_flat[2].tolist()\n",
    "#BSA_zones['support_no_response'] = partySup_flat[9].tolist() for some reason it does not like this line or any other DNR\n",
    "partySup_DNR_list = partySup_flat[9].tolist()\n",
    "BSA_zones['support_DNR'] = partySup_DNR_list\n",
    "\n",
    "# who do they support?\n",
    "BSA_zones['support_conservative'] = partySupWho_flat[1].tolist()\n",
    "BSA_zones['support_labour'] = partySupWho_flat[2].tolist()\n",
    "BSA_zones['support_libdem'] = partySupWho_flat[3].tolist()\n",
    "BSA_zones['support_green'] = partySupWho_flat[6].tolist()\n",
    "BSA_zones['support_ukip'] = partySupWho_flat[7].tolist()\n",
    "BSA_zones['support_brexit'] = partySupWho_flat[8].tolist()\n",
    "BSA_zones['support_other'] = partySupWho_flat[9].tolist()\n",
    "BSA_zones['support_none'] = partySupWho_flat[10].tolist()\n",
    "support_no_response_list = partySupWho_flat.loc[:,'98':].sum(axis=1).tolist()\n",
    "BSA_zones['support_no_response'] = support_no_response_list\n",
    "\n",
    "# are they interested in politics?\n",
    "BSA_zones['political_interest_yes'] = polInterest_flat.loc[:,'1':'3'].sum(axis=1).tolist()\n",
    "BSA_zones['political_interest_no'] = polInterest_flat.loc[:,'4':'5'].sum(axis=1).tolist()\n",
    "\n",
    "# opinions on welfare?\n",
    "BSA_zones['welfare_pro'] = welfare_flat[1].tolist()\n",
    "BSA_zones['welfare_anti'] = welfare_flat[3].tolist()\n",
    "BSA_zones['welfare_middle'] = welfare_flat[2].tolist()\n",
    "BSA_zones['welfare_no_response'] = welfare_flat[8].tolist()\n",
    "\n",
    "# opinions on wealth redistribution\n",
    "BSA_zones['wealth_redist_pro'] = redistrb_flat.loc[:,'1':'2'].sum(axis=1).tolist()\n",
    "BSA_zones['wealth_redist_anti'] = redistrb_flat.loc[:,'4':'5'].sum(axis=1).tolist()\n",
    "BSA_zones['wealth_redist_neither'] = redistrb_flat[3].tolist()\n",
    "\n",
    "# left, right or centrist?\n",
    "BSA_zones['lean_left'] = leftright2_flat[1].tolist()\n",
    "BSA_zones['lean_right'] = leftright2_flat[3].tolist()\n",
    "BSA_zones['lean_centrist'] = leftright2_flat[2].tolist()\n",
    "BSA_zones['lean_no_response'] = leftright2_flat[8].tolist()\n",
    "\n",
    "# libertarian or authoritarian\n",
    "BSA_zones['libertarian'] = libauth2_flat[1].tolist()\n",
    "BSA_zones['authoritarian'] = libauth2_flat[3].tolist()\n",
    "BSA_zones['neither_lib_auth'] = libauth2_flat[2].tolist()\n",
    "\n",
    "# what religion do they follow?\n",
    "BSA_zones['rel_christian'] = religion_flat.loc[:,'1':'3'].sum(axis=1).tolist()\n",
    "BSA_zones['rel_nonChristian'] = religion_flat[4].tolist()\n",
    "BSA_zones['rel_none'] = religion_flat[5].tolist()\n",
    "BSA_zones['rel_other'] = religion_flat[6].tolist()\n",
    "BSA_zones['rel_no_response'] = religion_flat[9].tolist()\n",
    "\n",
    "# what nationality do they best identify with?\n",
    "BSA_zones['nat_british'] = nationality_flat[1].tolist()\n",
    "BSA_zones['nat_english'] = nationality_flat[2].tolist()\n",
    "BSA_zones['nat_european'] = nationality_flat[3].tolist()\n",
    "BSA_zones['nat_irish'] = nationality_flat[4].tolist()\n",
    "BSA_zones['nat_scottish'] = nationality_flat[6].tolist()\n",
    "BSA_zones['nat_welsh'] = nationality_flat[8].tolist()\n",
    "#BSA_zones['nat_other_none'] = nationality_flat.loc[:,'9':'10'].sum(axis=1).tolist() # doesn't like this line\n",
    "# nat_other_none_list = nationality_flat.loc[:,'9':'10'].sum(axis=1).tolist() # doesn't like this line either\n",
    "\n",
    "# solution was this!\n",
    "list_nat1 = nationality_flat[9].tolist()\n",
    "list_nat2 = nationality_flat[10].tolist()\n",
    "list_nat3 = []\n",
    "for i in range(0,len(list_nat1)):\n",
    "    result = list_nat1[i]+list_nat2[i]\n",
    "    list_nat3.append(result)\n",
    "BSA_zones['nat_other_none'] = list_nat3\n",
    "\n",
    "BSA_zones['nat_no_reponse'] = nationality_flat.loc[:,'98':].sum(axis=1).tolist()\n",
    "\n",
    "# what's the racial origin they best identify with?\n",
    "BSA_zones['race_asian'] = raceOrigin_flat[2.0].tolist()\n",
    "BSA_zones['race_white'] = raceOrigin_flat[3.0].tolist()\n",
    "BSA_zones['race_mixed'] = raceOrigin_flat[4.0].tolist()\n",
    "BSA_zones['race_other'] = raceOrigin_flat[5.0].tolist()\n",
    "BSA_zones['race_no_response'] = raceOrigin_flat.loc[:,'8.0':].sum(axis=1).tolist()\n",
    "\n",
    "# do they have a disability that affects their daily life\n",
    "BSA_zones['disab_affect'] = disability_flat[1].tolist()\n",
    "BSA_zones['disab_no_affect'] = disability_flat[2].tolist()\n",
    "BSA_zones['disab_none'] = disability_flat[3].tolist()\n",
    "BSA_zones['disab_no_response'] = disability_flat[4].tolist()\n",
    "\n",
    "# who did they vote for in the last general election\n",
    "BSA_zones['voted_conservative'] = voteParty_flat[1].tolist()\n",
    "BSA_zones['voted_labour'] = voteParty_flat[2].tolist()\n",
    "BSA_zones['voted_libdem'] = voteParty_flat[3].tolist()\n",
    "BSA_zones['voted_green'] = voteParty_flat[6].tolist()\n",
    "BSA_zones['voted_UKIP'] = voteParty_flat[7].tolist()\n",
    "BSA_zones['voted_other'] = voteParty_flat[8].tolist()\n",
    "BSA_zones['voted_no_reponse'] = voteParty_flat.loc[:,'98':].sum(axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa44f71-f263-450b-83c9-839b63a98ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e66c4-401c-4f68-aee1-7dd19855be81",
   "metadata": {},
   "source": [
    "### Step 4: Validation and checking the data to ensure we've aggregated it correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6c636-a43a-44df-a471-016ecca01555",
   "metadata": {},
   "source": [
    "To ensure that we've aggregate and seperated the populations up correctly, we'll sum up each of our categorisations to ensure that it does represent York's population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b23ccd-efb9-4920-8e1b-f36b36b24661",
   "metadata": {},
   "outputs": [],
   "source": [
    "york_pop = []\n",
    "york_pop.append(BSA_zones['total_pop'].sum(axis=0))\n",
    "york_pop.append(BSA_zones.loc[:,'age1864':'age65'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'female':'male'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'support_yes':'support_DNR'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'support_conservative':'support_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'political_interest_yes':'political_interest_no'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'welfare_pro':'welfare_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'wealth_redist_pro':'wealth_redist_neither'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'lean_left':'lean_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'libertarian':'neither_lib_auth'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'rel_christian':'rel_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'nat_british':'nat_no_reponse'].sum().sum()) # doesn't like this one\n",
    "york_pop.append(BSA_zones.loc[:,'race_asian':'race_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'disab_affect':'disab_no_response'].sum().sum())\n",
    "york_pop.append(BSA_zones.loc[:,'voted_conservative':'voted_no_reponse'].sum().sum())\n",
    "\n",
    "print(york_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a91-f1df-4d43-8147-afc01618ce29",
   "metadata": {},
   "source": [
    "The list printed out above shows that the sums of the columns representing each category all add up to 162,096, which is our adult population of York! Therefore we can conclude the the data is aggregated correctly!\n",
    "\n",
    "Finally, we will check for NaN values in our dataframe; luckily there aren't any!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14095275-4c9d-4d60-ad6a-62eb53ed6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(BSA_zones.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1efd67-06d7-4092-8165-a8732d5ab73d",
   "metadata": {},
   "source": [
    "Whilst we do not need close to the number of columns we've created, it is a useful dataset to have and it can be used for other studies, and goes another step further to address the lack of data that this project seeks to aid. We'll save the aggregated dataframe as a CSV that can then be used for our geodemographics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2c13a-cf30-4ab4-93a1-9d91276de20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_zones.to_csv(\"data/BSA_agg.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
