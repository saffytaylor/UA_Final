{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d943e6-3af8-4238-934c-3be362d5b7f2",
   "metadata": {},
   "source": [
    "# Method 1: Spatial microsimulation\n",
    "\n",
    "Spatial microsimulation is a method that allows us to create synthetic data to represent a whole population when we don't actually have the data for a whole population. In this case, we're taking data from the British Social Attitudes Survey as our individual granular data, and combining it with census data to create our synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68632b-2e24-4d7d-af78-2d08c5a2b8dd",
   "metadata": {},
   "source": [
    "### Step 1: Importing all the necessary packages for our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a4700-ee45-46ad-80f2-c92474189501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from ipfn import ipfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b30a5-7ced-4a2e-a592-90c2df2e3065",
   "metadata": {},
   "source": [
    "### Step 2: Importing our individual-level BSA data and our constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73201abd-9175-4af1-8e5e-a91e87064ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020 = pd.read_csv('data/BSA2020/final_bsa_cleaned.csv', low_memory=False)\n",
    "con_age = pd.read_csv('data/census data/con_age.csv')\n",
    "con_sex = pd.read_csv('data/census data/con_sex.csv') # 1 represents female, 2 represents male"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6ecfd-da5c-4a81-a7c1-cac99923f964",
   "metadata": {},
   "source": [
    "We also need to check our constraints to ensure that they both have the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88dbc82-d488-4681-875b-cfbd76c3f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('con_age total: ',con_age['age_1864'].sum() + con_age['age_65'].sum())\n",
    "print('con_sex total: ',con_sex['sex_male'].sum() + con_sex['sex_female'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b9003-71fa-4ee1-8f52-6c70a1e7f645",
   "metadata": {},
   "source": [
    "The above calculations show that both constraints represent the same population size for adults in York."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8f794-adbc-4d17-beaa-41fc1c2d912c",
   "metadata": {},
   "source": [
    "### Step 3: Preparing the data for spatial microsimulation\n",
    "First, we will remove all of the BSA_2020 data that isn't related to out constraints. We only need age and sex to be able to allocate individuals to zones so we'll copy the data set and remove all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51eb0f2-775c-4e2b-8302-6b537f6276e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_original = BSA_2020.copy() # making a deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83d6d1-cf67-4063-aa9a-0ff92137c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all columns unnecessary for weighting\n",
    "BSA_2020 = BSA_2020.drop(['economic', \n",
    "                          'partySup', \n",
    "                          'partySupWho',\n",
    "                          'polInterest',\n",
    "                          'welfare',\n",
    "                          'redistrb',\n",
    "                          'leftright',\n",
    "                          'leftright2',\n",
    "                          'libauth',\n",
    "                          'libauth2',\n",
    "                          'religion',\n",
    "                          'nationality',\n",
    "                          'raceOrigin',\n",
    "                          'disability',\n",
    "                          'voteAct',\n",
    "                          'voteParty'\n",
    "                         ],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a91bf-678e-47b3-97c4-d9796aaf181c",
   "metadata": {},
   "source": [
    "Next we need to recategorise the BSA_2020 individual data into bins that match the categories in our constraints dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af021e48-6dc0-43f0-9948-8ebfe9a89f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020['age'] = pd.to_numeric(BSA_2020['age']) # ensuring age is numeric\n",
    "\n",
    "# changing age to categorical attribute, with categories matching the indexing of the age constraint\n",
    "BSA_2020['age'] = pd.cut(BSA_2020['age'], [17,64,120], labels = ['age_1864','age_65'])\n",
    "BSA_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697ab15-b018-4630-8595-d548e8dbb5ae",
   "metadata": {},
   "source": [
    "Let's do the same with the sex constraint, and rename the categories in the constraints dataframe to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcd90b-6da3-463f-8278-3aa866a38c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020['sex'] = pd.to_numeric(BSA_2020['sex'])\n",
    "\n",
    "# changing age to categorical attribute, with categories matching the indexing of the age constraint\n",
    "BSA_2020['sex'] = pd.cut(BSA_2020['sex'], [0,1,2], labels = ['f','m'])\n",
    "BSA_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b858734-2d6d-4c2b-aa68-f41f875f3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa0734-9682-4b5c-9085-f3c7acf44f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_sex = con_sex.rename(columns={'sex_female':'f','sex_male':'m'}) # all working and looking good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d257fe4-8b83-4332-9c3a-050911df86e6",
   "metadata": {},
   "source": [
    "Next we'll merge these constraints into one table together, merging it on geo_code to avoid duplicate columns and to match up the zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d56b3-f5ac-4607-91a4-84968415e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = con_sex.merge(con_age, left_on='geo_code', right_on='geo_code', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627afe5c-22c4-43a2-a2dd-59923db12d8b",
   "metadata": {},
   "source": [
    "Now that's all merged, but for the next step we also need to drop the geo_code column from the dataframe. We'll make a copy of the constraints dataframe as it is now and then drop the geo_code column from the dataframe we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c58c41-e9b7-4fd4-92c5-bf97ffcdc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_geo = constraints.copy() # making a deep copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d9e46-a6d6-4030-b6a6-fb8750a4a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = constraints.drop(columns=['geo_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e47bdd-b141-4d9f-9c1f-173742dc0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78650256-58f4-408d-bfa4-62fc425c3da7",
   "metadata": {},
   "source": [
    "This gives us one full table with all of our constaint variables, with the names of columns matching the individual level data we have in our BSA_2020 table. The next step to prep the data is to 'flatten' the individual level dataset, so it can be more easily compared with the constraints. This basically means that we turn the data into fields, and have the data entries themselves become boolean. We do this first individually for age and then for sex, and then we combine these into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6cc7e-fb31-4066-9ffb-6e680d1b7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of our two dataframes\n",
    "print (\"Shape of our BSA dataframe:\",BSA_2020.shape)\n",
    "print (\"Shape of our constraints:\",constraints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf2ea2-69de-42b4-b717-17fc026038ef",
   "metadata": {},
   "source": [
    "So we need these to have the same number of columns. At this time, our BSA dataframe has three columns and our constraints has four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26acc62-e389-4e79-89a5-66c55b03457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the age column\n",
    "age_flat = pd.pivot_table(BSA_2020,columns=['age'],values='id', index=BSA_2020.index, aggfunc=len, fill_value=0, observed=False)\n",
    "\n",
    "# flattening the sex column\n",
    "sex_flat = pd.pivot_table(BSA_2020,columns=['sex'],values='id', index=BSA_2020.index, aggfunc=len, fill_value=0, observed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6254-429f-48a7-8bcc-18556ae294df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge pivoted data to make flatten dataframe\n",
    "bsa_categorical = pd.DataFrame(age_flat.to_records()).merge(pd.DataFrame(sex_flat.to_records()),left_index=True,right_index=True)\n",
    "\n",
    "# the above line creates a dataframe with extra index columns, we don\n",
    "bsa_categorical = bsa_categorical.drop(['index_x','index_y'],axis=1)\n",
    "bsa_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e123ec-52c0-4fde-b820-b7403cd4d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the column sums to ensure that this is correct and saving these values for later\n",
    "bsa_agg = bsa_categorical.sum(axis=0)\n",
    "bsa_agg # age adds up to 275 (total number of individual data entries) and so does sex so it's worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d840fa7-6ece-47d7-b0e0-f2e5c4fad96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Shape of individual flattened dataframe:\",bsa_categorical.shape)\n",
    "print (\"Shape of constraints dataframe:\",constraints.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010fb86-76cf-42fa-9719-3fd2de5a8bcf",
   "metadata": {},
   "source": [
    "As we can see when the above code is run, our two dataframe now have the same dimensions and can be compared, as is demonstrated by the code below. The test dataframe created below shows us the first row of the constaints dataframe and the first row of the aggregated dataframe and confirms that both dataframes share the same dimesions and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1afd46-61b4-47f5-950e-dd1671fc08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([constraints.iloc[0], bsa_agg], axis=1).transpose()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6858f-d150-4f6a-8ef2-4dd0702e79d0",
   "metadata": {},
   "source": [
    "Now our data is all prepped, we can start working towards applying the microsimulation methods to create our synthetic population. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537673f-d6d8-45ef-940c-5a91ddc15be3",
   "metadata": {},
   "source": [
    "### Step 4: Iterative Proportional Fitting\n",
    "\n",
    "Whilst a lot of the coding for IPF can be done with python packages that we can import and use, the general gist of it is to generate weights to show how representative each individual is of each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10835c53-e81e-40d9-a4cb-1cab020ac482",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020_copy = BSA_2020.copy() # making another deep copy of BSA_2020 with the changes we made to variables\n",
    "BSA_2020_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42f79c-4b7b-4915-adb7-7cb9da6e046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020_copy.info() # looking at data types - we have two category types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3dbca-0a2e-4b3d-b589-bfac202d8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the categorical data into strings - the package we're using doesn't like category data!\n",
    "BSA_2020_copy['age'] = BSA_2020_copy['age'].astype(str)\n",
    "BSA_2020_copy['sex'] = BSA_2020_copy['sex'].astype(str)\n",
    "BSA_2020_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae73bf6-67ea-47b4-a7f9-66bed4b4114c",
   "metadata": {},
   "source": [
    "Now our BSA_2020_copy data is in the correct datatype, we can use a for loop to add columns with the caluculated weights. Increasing the efficiency from the example provided within the IPF guide page, we can do this with just four lines of code.\n",
    "1. The first line initiates the loop, and instructs it to repeat for the length of the contstraints dataframe (the number of electoral zones in York).\n",
    "2. The second line (and the first within the for loop) creates a new column (data-type float) containing 1.0 for each individual to represent the weighting for the current zone.\n",
    "3. The third carries out the IPF analysis, the key inputs being the dataframe itself, the aggregates (which don't need to be predefined as they can be defined within this line), and the dimensions (the column names we are working with).\n",
    "4. The final line adds the results of the previous calculation into our BSA_2020_copy dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24606254-587a-4407-9712-bca5d5997d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(constraints)):\n",
    "    # creates a column with float value 1.0 to hold that zone's weight\n",
    "    BSA_2020_copy['weight_' + str(i)] = np.ones(275) \n",
    "    \n",
    "    # carries out the ipf analysis using the dataframe, the aggregates (the constraints database), and the dimensions (sex and age)\n",
    "    ipf = ipfn.ipfn(BSA_2020_copy, [constraints.iloc[i,[0,1]], constraints.iloc[i,[2,3]]],[['sex'],['age']],weight_col='weight_'+str(i),convergence_rate = 1e-15)\n",
    "    \n",
    "    BSA_2020_copy = ipf.iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c23a0-342a-4e9a-9824-616f90d402a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020_copy # look at all those weights we've calculated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ffbf7-9176-4117-9a7f-1bc947db2bbf",
   "metadata": {},
   "source": [
    "Now we need to test that these weights we've calculated are actually correct and make sense with what our constraints tell us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bcfb0-3d0b-471d-b2dc-db0cf32112fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating marginal distribution of individuals \n",
    "bsa_agg0 = constraints.apply(lambda x: 1.0*bsa_agg, axis=1)\n",
    "\n",
    "bsa_agg_final = (bsa_agg0 * np.nan).copy()\n",
    "\n",
    "for i in range(0,len(constraints)):\n",
    "    bsa_agg_final.iloc[i] = bsa_categorical.apply(lambda x: x*BSA_2020_copy['weight_'+str(i)],axis=0).sum(axis=0)\n",
    "\n",
    "bsa_agg_final # this comes out in a funny order so we'll swap the columns to match our constraints for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34912f6-1288-4684-8abd-362ea02dfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsa_agg_final_ordered = bsa_agg_final.reindex(columns=['m', 'f', 'age_1864', 'age_65'])\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html\n",
    "bsa_agg_final_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303d34f-4b2a-4600-aab0-fd3bd3522cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce528a77-d663-4867-aed8-5c8ed9a1d33c",
   "metadata": {},
   "source": [
    "The output from our marginal distribution matches our constraints, so we can conclude that the weights we've calculated are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42c405-e5e2-4226-8219-5e01b7afb6a2",
   "metadata": {},
   "source": [
    "### Step 5: Intergerisation using the TRS method\n",
    "\n",
    "Intergerisation is the process by which our float values become integers corresponding to the number of individuals within our synthetic population. To do this, we will use the TRS ('trucate replicate sample') method proposed by Lovelace and Ballas (2013).\n",
    "might want to **rename certain variables here as has been pretty much copied across**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1a597-e95f-4dac-b653-0531dabff338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intergerisation (Lovelace and Ballas, 2013)\n",
    "def int_trs(weights):\n",
    "    xv = np.array(weights).ravel() # convert to vector if necessary\n",
    "    xint = np.floor(xv) # truncate - get the integer part of the result\n",
    "    r = xv - xint # get the decimal of the weight\n",
    "    frac_sum = round(r.sum()) # work out deficit population\n",
    "    xs = np.random.choice(len(xv),int(frac_sum),True,r/r.sum()) # sample based on deficit\n",
    "    topup = np.bincount(xs,minlength=len(xv)) # result of the sample deficit\n",
    "    return xint + topup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cc17c-48e8-4dd3-a257-49e2875e883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expansion\n",
    "def int_expansion(weights):\n",
    "    return np.repeat(range(0,len(weights)),weights.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b30cdb-d4ff-49fb-a5b6-6a2962e88d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting this all together!\n",
    "\n",
    "individuals = [] # creating a list to hold the results of integerisation and expansion\n",
    "\n",
    "for i in range(0, len(constraints)):\n",
    "    # calls both functions in one line to integerise and expand\n",
    "    integerise = int_expansion(int_trs(BSA_2020_copy['weight_' + str(i)]))\n",
    "    current_ind = BSA_original.loc[integerise].copy() # Select the relevant individuals using .loc\n",
    "    current_ind['zone'] = i     # Assigning the 'zone' - allocates individuals to different zones\n",
    "    individuals.append(current_ind)\n",
    "\n",
    "BSA_2020_synthetic = pd.concat(individuals) # concatonating the resulting list into a new dataframe\n",
    "BSA_2020_synthetic.reset_index(drop=True, inplace=True) #Resetting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c937059-624f-494d-998a-97a4e5c43511",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_2020_synthetic # looking at our new synthetic population of York!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374aa1dd-720f-4e2b-a69a-e01b39194102",
   "metadata": {},
   "source": [
    "Now we'll add the geo_codes column back in to our new synthetic dataframe, so we have a spatial element that we can connect to our shapefile in order to perform our geodemographics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849d934-fb45-491e-9687-fe3123442f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "constraints_geo_copy = constraints_geo.copy()\n",
    "constraints_geo = constraints_geo.drop(columns=['m','f','age_1864','age_65'])\n",
    "constraints_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f677841-62c4-448c-9f79-9a614540004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_synthetics_geo = pd.merge(BSA_2020_synthetic,constraints_geo, left_on='zone', right_index=True, validate='many_to_one')\n",
    "    # (pandas, 2024)\n",
    "    # https://pandas.pydata.org/docs/dev/reference/api/pandas.merge.html#pandas.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ec756-6d71-47be-947b-aa848971d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the zone column too for good measyre\n",
    "BSA_synthetics_geo = BSA_synthetics_geo.drop(columns=['zone'])\n",
    "BSA_synthetics_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee9a89-099f-4fe2-8ead-2b9f0cf90cd3",
   "metadata": {},
   "source": [
    "Now we have complete synthetic population representing BSA results for the whole of York, and including geo_code zones that we can use to map the data. We'll save our new dataframe as a CVS at this point, and that is the spatial microsimulation part of the project all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2e077-06d4-4402-b559-533a40e5db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSA_synthetics_geo.to_csv(\"data/BSA_synthetics_geo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
